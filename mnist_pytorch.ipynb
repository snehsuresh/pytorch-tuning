{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goj7Pr6qsoSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPU6USjWs1y0",
        "outputId": "e0c727fc-057e-4dfa-9b16-45b49414c252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T5m52nrotAgo",
        "outputId": "20cb8443-5c09-471d-c20c-82152f18cc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('T-shirt/top', 'Trouser/pants','Pullover shirt','Dress','Coat','Sandal',\n",
        "           'Shirt','Sneaker','Bag','Ankle boot')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7OvdSoiuq7N",
        "outputId": "9f48882c-21ec-4989-ffb6-23924785b602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12322978.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 193725.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3591582.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21378492.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = iter(trainset)\n",
        "\n",
        "image, label = next(train_iter)\n",
        "\n",
        "image.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL-H7xl5tLFS",
        "outputId": "a2734cff-e04f-4400-ca8a-d1607c8996e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.min(image).item(), torch.max(image).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJadfWUTkTmQ",
        "outputId": "5224db54-164c-46d7-89c5-1d840dfd1a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What it did:**\n",
        "This code retrieves the minimum and maximum pixel values in the image tensor, which represents a FashionMNIST image. Let's break it down:\n",
        "\n",
        "**torch.min(image):**\n",
        "\n",
        "This computes the minimum value within the image tensor.\n",
        "In a tensor representing an image, each element represents the intensity of a pixel. Since FashionMNIST images are grayscale, the pixel values will be between 0 and 1 if normalized, or between 0 and 255 if not normalized.\n",
        "\n",
        "**torch.max(image):**\n",
        "\n",
        "Similarly, this computes the maximum value within the image tensor, showing the pixel with the highest intensity.\n",
        "\n",
        "**.item():**\n",
        "\n",
        "Both torch.min(image) and torch.max(image) return tensor objects.\n",
        "The .item() method converts these tensor values into Python scalar values (regular numbers), which makes them easier to print and work with.\n"
      ],
      "metadata": {
        "id": "nPmN8hqBkEXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np_img = image.numpy()\n",
        "print(classes[label])\n",
        "plt.imshow(np_img.reshape((28, 28, 1))) #In pytorch the challel dimension is first (1,28,28), but for matplotlib the channel dimension should be last for RGB, but shouldnt exist at all for grayscale images (28,28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "SQi42rzJjoHl",
        "outputId": "c16de2d3-fba7-4e09-8acf-774e6e3f4a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ankle boot\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x785bf00664d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAilUlEQVR4nO3df3DU9b3v8dfm1xIg2RBCfknAgAoqEFsKMdVSlFwgnesF5fRq650DvY4eaXCK9IdDj4r2dE5anGO9tVTvndNCnSnaOlfkyLHcKjShtGALwqXWNgdoFCwk/KjZDQlJNtnP/YNrNArC+8smnyQ8HzM7Q3a/L74fvnyTV77Z3XdCzjknAAD6WYrvBQAALk0UEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAv0nwv4MMSiYSOHDmirKwshUIh38sBABg559TS0qLi4mKlpJz7OmfAFdCRI0dUUlLiexkAgIt0+PBhjR079pyPD7gCysrKkiTdqM8pTemeVwMAsOpSXNv1cs/X83PpswJas2aNHnvsMTU2NqqsrExPPvmkZs6ced7cez92S1O60kIUEAAMOv9/wuj5nkbpkxch/OxnP9OKFSu0atUqvf766yorK9O8efN07NixvtgdAGAQ6pMCevzxx3X33XfrS1/6kq655ho9/fTTGj58uH784x/3xe4AAINQ0guos7NTu3fvVmVl5fs7SUlRZWWlduzY8ZHtOzo6FIvFet0AAENf0gvoxIkT6u7uVkFBQa/7CwoK1NjY+JHta2pqFIlEem68Ag4ALg3e34i6cuVKRaPRntvhw4d9LwkA0A+S/iq4vLw8paamqqmpqdf9TU1NKiws/Mj24XBY4XA42csAAAxwSb8CysjI0PTp07Vly5ae+xKJhLZs2aKKiopk7w4AMEj1yfuAVqxYocWLF+tTn/qUZs6cqSeeeEKtra360pe+1Be7AwAMQn1SQLfffruOHz+uhx9+WI2Njbruuuu0efPmj7wwAQBw6Qo555zvRXxQLBZTJBLRbC1gEgIADEJdLq5abVQ0GlV2dvY5t/P+KjgAwKWJAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeJHmewHAgBIK2TPOJX8dZ5E6OteceXfeVYH2lb1+Z6CcWYDjHUpLN2dcvNOcGfCCnKtB9dE5zhUQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHjBMFLgA0KpqeaM6+oyZ1Kuu8ac+dM/jLTv57Q5IklKb51pzqSdTtj388td5ky/DhYNMiw1wDmkkP1aoD+PQyjNVhUh56QL+LTgCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvGAYKfAB1qGLUrBhpIfn5Zgzd1b82pz5zfEJ5owkvR0uNGdcpn0/aZUV5sxVP/yrOdP11iFzRpLknD0S4HwIInXUqGDB7m57JBYzbe/chR0DroAAAF5QQAAAL5JeQI888ohCoVCv2+TJk5O9GwDAINcnzwFde+21evXVV9/fSYCfqwMAhrY+aYa0tDQVFtqfxAQAXDr65Dmg/fv3q7i4WBMmTNCdd96pQ4fO/QqUjo4OxWKxXjcAwNCX9AIqLy/XunXrtHnzZj311FNqaGjQZz7zGbW0tJx1+5qaGkUikZ5bSUlJspcEABiAkl5AVVVV+vznP69p06Zp3rx5evnll9Xc3Kyf//znZ91+5cqVikajPbfDhw8ne0kAgAGoz18dkJOTo6uuukoHDhw46+PhcFjhcLivlwEAGGD6/H1Ap06d0sGDB1VUVNTXuwIADCJJL6Cvfe1rqqur01tvvaXf/va3uvXWW5WamqovfOELyd4VAGAQS/qP4N555x194Qtf0MmTJzVmzBjdeOON2rlzp8aMGZPsXQEABrGkF9Bzzz2X7L8S6DeJ9vZ+2U/nJ06ZM38X2WXODEuJmzOSVJeSMGf+utX+Ctbuafbj8PbjWeZMYs+nzRlJGv2GfXBn9p6j5syJWZeZM8en2welSlLBTntm1KsHTdu7RKd04vzbMQsOAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALzo819IB3gRCgXLOfuAx1P/9Xpz5u+vqTVnDsbtE+XHZvzNnJGkzxfvtof+mz3zg/rPmjOtf4mYMykjgg3ubLze/j36XxfY/59cvMucGfV6sC/fKYubzJlY5wTT9l3xdmnjBazFvBIAAJKAAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAAL5iGjf4VdEr1AHb9A78zZ24a+WYfrOSjLlOwKdCtLsOcae4eYc6suubfzZnjV2WZM3EX7Evdv+7/tDlzKsC07tQu++fF9f99jzkjSYtyf2/OrP7fU03bd7n4BW3HFRAAwAsKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAeMEwUvQvF2w45kC2/1S+OXMye6Q509iVY86MTj1lzkhSVsppc+by9BPmzPFu+2DR1PSEOdPpUs0ZSXr02pfMmfar082Z9FC3OfPpYUfMGUn6/Jt/b86M0F8C7et8uAICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8YRgpcpDFh+8DPYaG4OZMR6jJnjsRHmTOStP/0JHPmP2L2oazzC/5ozsQDDBZNVbAhuEGGhBanv2vOtDv7AFP7GXTGDQX2waJ7A+7rfLgCAgB4QQEBALwwF9C2bdt0yy23qLi4WKFQSC+++GKvx51zevjhh1VUVKTMzExVVlZq//79yVovAGCIMBdQa2urysrKtGbNmrM+vnr1an3/+9/X008/rddee00jRozQvHnz1N7eftGLBQAMHeYXIVRVVamqquqsjznn9MQTT+jBBx/UggULJEnPPPOMCgoK9OKLL+qOO+64uNUCAIaMpD4H1NDQoMbGRlVWVvbcF4lEVF5erh07dpw109HRoVgs1usGABj6klpAjY2NkqSCgoJe9xcUFPQ89mE1NTWKRCI9t5KSkmQuCQAwQHl/FdzKlSsVjUZ7bocPH/a9JABAP0hqARUWFkqSmpqaet3f1NTU89iHhcNhZWdn97oBAIa+pBZQaWmpCgsLtWXLlp77YrGYXnvtNVVUVCRzVwCAQc78KrhTp07pwIEDPR83NDRo7969ys3N1bhx47R8+XJ9+9vf1pVXXqnS0lI99NBDKi4u1sKFC5O5bgDAIGcuoF27dummm27q+XjFihWSpMWLF2vdunX6xje+odbWVt1zzz1qbm7WjTfeqM2bN2vYsGHJWzUAYNALOeeCTenrI7FYTJFIRLO1QGkh+4A+DHChkD2Sah8+6brsgzslKXWUfXjnHTv+YN9PyP5pd7wry5zJSW0zZySprtk+jPSPJ8/+PO/H+dakfzNnXm+73JwpzrAPCJWCHb+3OvPMmSvDZ3+V8Mf5xbtl5owklQz7mznzy+WzTNt3dbVre+2jikajH/u8vvdXwQEALk0UEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4Yf51DMBFCTB8PZRmP02DTsM+fNfV5szNw18yZ37bfpk5MyatxZyJO/skcUkqCkfNmayCdnOmuXu4OZObdsqcaenONGckaXhKhzkT5P/pkxknzJn7X/2kOSNJWVNOmjPZ6bZrlcQFXttwBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXjCMFP0qlJ5hziTa7UMug8r7Q6c5c6I73ZzJSWkzZzJC3eZMZ8BhpJ/ObTBnjgcY+Pn66VJzJiv1tDkzJsU+IFSSStLtgzv/0F5izrzceoU5c9d/ftWckaRn/9d/MmcyNv/WtH2Ki1/YduaVAACQBBQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADw4tIeRhoKBYul2YdPhlIDdH2KPZNo77DvJ2EfchmUi9uHffan//E/f2DOHO7KMWca4/ZMTqp9gGm3gp3jO09HzJlhKRc2gPKDxqTFzJlYwj70NKiWxDBzJh5gAGyQY/fA6P3mjCS9EK0MlOsLXAEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBdDZhhpKM3+T3FdXYH2FWSgprPPGhySTi+Yac4cXmgflnrnJ35nzkhSY1eWObOn7XJzJpJ62pwZkWIfNNvu7INzJelI5yhzJshAzdy0U+ZMfoABpt0u2Pfaf43bj0MQQQbNvtNlP3aS1PJfWsyZnGcC7eq8uAICAHhBAQEAvDAX0LZt23TLLbeouLhYoVBIL774Yq/HlyxZolAo1Os2f/78ZK0XADBEmAuotbVVZWVlWrNmzTm3mT9/vo4ePdpze/bZZy9qkQCAocf8zH1VVZWqqqo+dptwOKzCwsLAiwIADH198hxQbW2t8vPzNWnSJC1dulQnT54857YdHR2KxWK9bgCAoS/pBTR//nw988wz2rJli7773e+qrq5OVVVV6u4++0tpa2pqFIlEem4lJSXJXhIAYABK+vuA7rjjjp4/T506VdOmTdPEiRNVW1urOXPmfGT7lStXasWKFT0fx2IxSggALgF9/jLsCRMmKC8vTwcOHDjr4+FwWNnZ2b1uAIChr88L6J133tHJkydVVFTU17sCAAwi5h/BnTp1qtfVTENDg/bu3avc3Fzl5ubq0Ucf1aJFi1RYWKiDBw/qG9/4hq644grNmzcvqQsHAAxu5gLatWuXbrrppp6P33v+ZvHixXrqqae0b98+/eQnP1Fzc7OKi4s1d+5c/dM//ZPC4XDyVg0AGPRCzjnnexEfFIvFFIlENFsLlBYKNkhxIEorsr8vKl5aYM787erh5kxbYcickaTrPvcnc2ZJwXZz5ni3/XnB9FCwQbMt3ZnmTGF6szmzNXqNOTMyzT6MNMjQU0n6ZOZb5kxzwn7uFae9a848cODvzJmC4fYBnJL0r+NfNmfiLmHO1Mft36BnpdiHIkvSr9uuMGc2XDPGtH2Xi6tWGxWNRj/2eX1mwQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMCLpP9Kbl86qmaYM/n/+JdA+7ou+x1z5ppM+xTo9oR9GviwlLg58+bpy8wZSWpLZJgz+zvtU8GjXfYpy6kh+0RiSTrWmWXO/EtDpTmzZebT5syDR+abMymZwYbdn+weac4sGhkLsCf7Of4P47aZMxMyjpkzkrSp1f6LNI/ER5kzBelRc+by9OPmjCTdlvUf5swG2aZhXyiugAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADAiwE7jDSUlqZQ6MKXV/7PvzfvY07WH80ZSWpzYXMmyGDRIEMNg4iktQXKdcTtp8+xeHagfVldFW4MlLs1e685s+0H5ebMje33mTMHb15rzmw5nWrOSNLxLvv/0x0NN5szrx8qMWeuv7zBnJma9VdzRgo2CDcrtd2cSQ91mTOtCfvXIUna2W4fNNtXuAICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8G7DDSo0unKzU87IK3fyTypHkf6/92vTkjSSXD/mbOjM84Yc6UZb5tzgSRlWIfnihJk7LtAxQ3tY41Z2qbJ5szRenN5owk/bptojnz3COPmTNL7v+qOVPx8r3mTOzyYN9jdo1w5kx22Ulz5sFP/Ls5kxHqNmeau+1DRSUpN9xqzuSkBhvuaxVkKLIkZaWcNmdSJ11h2t51d0j7z78dV0AAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMALCggA4MWAHUY6/FhCqRmJC95+U+w68z4mZB43ZyTpRDzLnPk/p6aaM2Mz3zVnIqn2QYNXhBvNGUna255jzmw+fq05U5wZM2ea4hFzRpJOxkeYM20J+1DIH33vcXPmX5oqzZlbc183ZySpLMM+WLQ5Yf9+9s3OQnOmJXHhQ4rf0+7SzRlJigYYYpoV4HMw7uxfilPdhX99/KCcFPuw1NjU0abtu+LtDCMFAAxcFBAAwAtTAdXU1GjGjBnKyspSfn6+Fi5cqPr6+l7btLe3q7q6WqNHj9bIkSO1aNEiNTU1JXXRAIDBz1RAdXV1qq6u1s6dO/XKK68oHo9r7ty5am19/5c23X///XrppZf0/PPPq66uTkeOHNFtt92W9IUDAAY30zNfmzdv7vXxunXrlJ+fr927d2vWrFmKRqP60Y9+pPXr1+vmm2+WJK1du1ZXX321du7cqeuvD/YbSAEAQ89FPQcUjUYlSbm5uZKk3bt3Kx6Pq7Ly/VfrTJ48WePGjdOOHTvO+nd0dHQoFov1ugEAhr7ABZRIJLR8+XLdcMMNmjJliiSpsbFRGRkZysnJ6bVtQUGBGhvP/lLfmpoaRSKRnltJSUnQJQEABpHABVRdXa033nhDzz333EUtYOXKlYpGoz23w4cPX9TfBwAYHAK9EXXZsmXatGmTtm3bprFjx/bcX1hYqM7OTjU3N/e6CmpqalJh4dnfcBYOhxUO29/IBwAY3ExXQM45LVu2TBs2bNDWrVtVWlra6/Hp06crPT1dW7Zs6bmvvr5ehw4dUkVFRXJWDAAYEkxXQNXV1Vq/fr02btyorKysnud1IpGIMjMzFYlEdNddd2nFihXKzc1Vdna27rvvPlVUVPAKOABAL6YCeuqppyRJs2fP7nX/2rVrtWTJEknS9773PaWkpGjRokXq6OjQvHnz9MMf/jApiwUADB0h55zzvYgPisViikQimnXjQ0pLu/ChgzOe2G3e1xuxYnNGkgqGtZgz00a+Y87Ut9kHNR45nW3ODE+LmzOSlJlqz3U5++te8sP24z0ubB+mKUlZKfZBkhmhbnOmO8Drf67NOGLOHOoaZc5IUmNXjjnzZpv982lUmn0w5h8CfN62dWWYM5LU0W1/mry9y56JhNvNmRm5b5szkpQi+5f89f/2WdP2ifZ2/eXb/6hoNKrs7HN/TWIWHADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALwI9BtR+0PK9n1KCaVf8PbP//IG8z4eWvC8OSNJdc2TzZlNjVPNmVin/TfFjhneas5kp9unTUtSbrp9X5EA04+HhbrMmXe7RpgzktSRcuHn3Hu6FTJnGjsi5sxvEleaM/FEqjkjSR0BckGmo/+tM8+cKc6MmjMtXRc+Wf+D3mrJNWdOREeaM+3D7V+Kt3dPNGckaX7hH82ZzGO2c7y748K25woIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALwIOeec70V8UCwWUyQS0WwtUJphGGkQ0TuvD5Sb8OV6c2ZmToM583psnDlzKMDwxHgi2Pch6SkJc2Z4eqc5MyzAkMuM1G5zRpJSZP90SAQYRjoi1X4cRqR1mDPZae3mjCRlpdpzKSH7+RBEaoD/o99FL0/+Qs4hK8D/U5ezfw5WRA6aM5L044ZPmzORzx0wbd/l4qrVRkWjUWVnZ59zO66AAABeUEAAAC8oIACAFxQQAMALCggA4AUFBADwggICAHhBAQEAvKCAAABeUEAAAC8oIACAFxQQAMCLgTuMNOU22zDSRLDhk/2ldVG5OVP+zd/bM1n2AYWTM5rMGUlKl3345LAAAytHpNiHfbYHPK2DfEe2/XSJOdMdYE9b373anIkHGHIpSU1t5x4geS7pAQfAWiWc/Xw43RVssHH09DBzJjXFfu611+aZM6PftA/plaTwy/avK1YMIwUADGgUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8GLgDiPVAtswUgQWmjE1UO50YaY5Ez7ZYc60jLfvJ/tgqzkjSSkdXeZM4v/+KdC+gKGKYaQAgAGNAgIAeGEqoJqaGs2YMUNZWVnKz8/XwoULVV9f32ub2bNnKxQK9brde++9SV00AGDwMxVQXV2dqqurtXPnTr3yyiuKx+OaO3euWlt7/7z97rvv1tGjR3tuq1evTuqiAQCDX5pl482bN/f6eN26dcrPz9fu3bs1a9asnvuHDx+uwsLC5KwQADAkXdRzQNFoVJKUm5vb6/6f/vSnysvL05QpU7Ry5Uq1tbWd8+/o6OhQLBbrdQMADH2mK6APSiQSWr58uW644QZNmTKl5/4vfvGLGj9+vIqLi7Vv3z498MADqq+v1wsvvHDWv6empkaPPvpo0GUAAAapwO8DWrp0qX7xi19o+/btGjt27Dm327p1q+bMmaMDBw5o4sSJH3m8o6NDHR3vvzckFouppKSE9wH1I94H9D7eBwRcvAt9H1CgK6Bly5Zp06ZN2rZt28eWjySVl5dL0jkLKBwOKxwOB1kGAGAQMxWQc0733XefNmzYoNraWpWWlp43s3fvXklSUVFRoAUCAIYmUwFVV1dr/fr12rhxo7KystTY2ChJikQiyszM1MGDB7V+/Xp97nOf0+jRo7Vv3z7df//9mjVrlqZNm9Yn/wAAwOBkKqCnnnpK0pk3m37Q2rVrtWTJEmVkZOjVV1/VE088odbWVpWUlGjRokV68MEHk7ZgAMDQYP4R3McpKSlRXV3dRS0IAHBpCPwybAwd7vd/CJQbluR1nEv2b/tpR5IS/bcr4JLHMFIAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAvKCAAgBcUEADACwoIAOAFBQQA8IICAgB4QQEBALyggAAAXlBAAAAv0nwv4MOcc5KkLsUl53kxAACzLsUlvf/1/FwGXAG1tLRIkrbrZc8rAQBcjJaWFkUikXM+HnLnq6h+lkgkdOTIEWVlZSkUCvV6LBaLqaSkRIcPH1Z2dranFfrHcTiD43AGx+EMjsMZA+E4OOfU0tKi4uJipaSc+5meAXcFlJKSorFjx37sNtnZ2Zf0CfYejsMZHIczOA5ncBzO8H0cPu7K5z28CAEA4AUFBADwYlAVUDgc1qpVqxQOh30vxSuOwxkchzM4DmdwHM4YTMdhwL0IAQBwaRhUV0AAgKGDAgIAeEEBAQC8oIAAAF4MmgJas2aNLr/8cg0bNkzl5eX63e9+53tJ/e6RRx5RKBTqdZs8ebLvZfW5bdu26ZZbblFxcbFCoZBefPHFXo875/Twww+rqKhImZmZqqys1P79+/0stg+d7zgsWbLkI+fH/Pnz/Sy2j9TU1GjGjBnKyspSfn6+Fi5cqPr6+l7btLe3q7q6WqNHj9bIkSO1aNEiNTU1eVpx37iQ4zB79uyPnA/33nuvpxWf3aAooJ/97GdasWKFVq1apddff11lZWWaN2+ejh075ntp/e7aa6/V0aNHe27bt2/3vaQ+19raqrKyMq1Zs+asj69evVrf//739fTTT+u1117TiBEjNG/ePLW3t/fzSvvW+Y6DJM2fP7/X+fHss8/24wr7Xl1dnaqrq7Vz50698sorisfjmjt3rlpbW3u2uf/++/XSSy/p+eefV11dnY4cOaLbbrvN46qT70KOgyTdfffdvc6H1atXe1rxObhBYObMma66urrn4+7ubldcXOxqamo8rqr/rVq1ypWVlflehleS3IYNG3o+TiQSrrCw0D322GM99zU3N7twOOyeffZZDyvsHx8+Ds45t3jxYrdgwQIv6/Hl2LFjTpKrq6tzzp35v09PT3fPP/98zzZ/+tOfnCS3Y8cOX8vscx8+Ds4599nPftZ95Stf8beoCzDgr4A6Ozu1e/duVVZW9tyXkpKiyspK7dixw+PK/Ni/f7+Ki4s1YcIE3XnnnTp06JDvJXnV0NCgxsbGXudHJBJReXn5JXl+1NbWKj8/X5MmTdLSpUt18uRJ30vqU9FoVJKUm5srSdq9e7fi8Xiv82Hy5MkaN27ckD4fPnwc3vPTn/5UeXl5mjJlilauXKm2tjYfyzunATeM9MNOnDih7u5uFRQU9Lq/oKBAf/7znz2tyo/y8nKtW7dOkyZN0tGjR/Xoo4/qM5/5jN544w1lZWX5Xp4XjY2NknTW8+O9xy4V8+fP12233abS0lIdPHhQ3/zmN1VVVaUdO3YoNTXV9/KSLpFIaPny5brhhhs0ZcoUSWfOh4yMDOXk5PTadiifD2c7DpL0xS9+UePHj1dxcbH27dunBx54QPX19XrhhRc8rra3AV9AeF9VVVXPn6dNm6by8nKNHz9eP//5z3XXXXd5XBkGgjvuuKPnz1OnTtW0adM0ceJE1dbWas6cOR5X1jeqq6v1xhtvXBLPg36ccx2He+65p+fPU6dOVVFRkebMmaODBw9q4sSJ/b3MsxrwP4LLy8tTamrqR17F0tTUpMLCQk+rGhhycnJ01VVX6cCBA76X4s175wDnx0dNmDBBeXl5Q/L8WLZsmTZt2qRf/epXvX59S2FhoTo7O9Xc3Nxr+6F6PpzrOJxNeXm5JA2o82HAF1BGRoamT5+uLVu29NyXSCS0ZcsWVVRUeFyZf6dOndLBgwdVVFTkeynelJaWqrCwsNf5EYvF9Nprr13y58c777yjkydPDqnzwzmnZcuWacOGDdq6datKS0t7PT59+nSlp6f3Oh/q6+t16NChIXU+nO84nM3evXslaWCdD75fBXEhnnvuORcOh926devcm2++6e655x6Xk5PjGhsbfS+tX331q191tbW1rqGhwf3mN79xlZWVLi8vzx07dsz30vpUS0uL27Nnj9uzZ4+T5B5//HG3Z88e9/bbbzvnnPvOd77jcnJy3MaNG92+ffvcggULXGlpqTt9+rTnlSfXxx2HlpYW97Wvfc3t2LHDNTQ0uFdffdV98pOfdFdeeaVrb2/3vfSkWbp0qYtEIq62ttYdPXq059bW1tazzb333uvGjRvntm7d6nbt2uUqKipcRUWFx1Un3/mOw4EDB9y3vvUtt2vXLtfQ0OA2btzoJkyY4GbNmuV55b0NigJyzrknn3zSjRs3zmVkZLiZM2e6nTt3+l5Sv7v99ttdUVGRy8jIcJdddpm7/fbb3YEDB3wvq8/96le/cpI+clu8eLFz7sxLsR966CFXUFDgwuGwmzNnjquvr/e76D7wccehra3NzZ07140ZM8alp6e78ePHu7vvvnvIfZN2tn+/JLd27dqebU6fPu2+/OUvu1GjRrnhw4e7W2+91R09etTfovvA+Y7DoUOH3KxZs1xubq4Lh8PuiiuucF//+tddNBr1u/AP4dcxAAC8GPDPAQEAhiYKCADgBQUEAPCCAgIAeEEBAQC8oIAAAF5QQAAALyggAIAXFBAAwAsKCADgBQUEAPCCAgIAePH/AIe0yFA5VNd3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**matplotlib.pyplot**\n",
        "Used for plotting and visualizing data.\n",
        "\n",
        "**numpy**: A powerful library for handling arrays and matrices.\n",
        "\n",
        "`image.numpy()`\n",
        "\n",
        "This converts the PyTorch tensor image into a NumPy array. PyTorch stores images as tensors, but for visualization with matplotlib, you'll need to convert it to a NumPy array.\n",
        "\n",
        "If image is a PyTorch tensor of shape (1, 28, 28) (grayscale image with one channel), this will convert it to a NumPy array with the same shape.\n",
        "\n",
        "> *NOTE* If image is on the GPU, you must move it to the CPU before converting to NumPy\n",
        "\n",
        "```\n",
        "np_img = image.cpu().numpy()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "My_c6Le5kmFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])"
      ],
      "metadata": {
        "id": "EWGKbKs0q3Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainset), len(valset), len(testset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFswfvB7q4H_",
        "outputId": "95c75b1e-bdf4-4d95-ec51-60d114c50131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of batches in the training set: {int(50000 / batch_size)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vZ7_7y2q8hV",
        "outputId": "1ee62cf3-e624-4d42-c524-ae93cd1682d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in the training set: 6250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of batches in the validation set: {int(10000 / batch_size)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGTXX_kJrA9V",
        "outputId": "23c350ac-e410-4c82-e854-d1a4a95aa9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in the validation set: 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "UyUSt1RFrEJB",
        "outputId": "8aedeaca-f87c-4de6-c92b-61833d087624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.utils.data.dataset.Subset</b><br/>def __init__(dataset: Dataset[T_co], indices: Sequence[int]) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py</a>Subset of a dataset at specified indices.\n",
              "\n",
              "Args:\n",
              "    dataset (Dataset): The whole Dataset\n",
              "    indices (sequence): Indices in the whole set selected for subset</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 393);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "An5xXzH9tl8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up data loaders**\n",
        "\n",
        "```\n",
        "torch.utils.data.DataLoader\n",
        "```\n",
        " A utility that allows you to **load data in batches and perform shuffling and parallel data loading using workers** (CPU threads).\n",
        "\n"
      ],
      "metadata": {
        "id": "C-VxM3yTtnOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "0GkCkaTtrF_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding 2D Convolution in CNNs\n",
        "\n",
        "## 1. What is a 2D Convolution?\n",
        "A **2D convolution** is a mathematical operation used primarily in image processing and convolutional neural networks (CNNs) to extract features from images. It involves a sliding window (filter or kernel) that moves across the input image to produce a feature map.\n",
        "\n",
        "### How it Works:\n",
        "- **Input Image**: An image is typically represented as a 2D array of pixel values. For a grayscale image, it has dimensions `(height, width)`, and for a color image (like RGB), it has dimensions `(height, width, channels)`.\n",
        "- **Convolution Filter (Kernel)**: This is a smaller 2D array (e.g., `3x3` or `5x5`) of weights that is applied to the input image to detect features (like edges, textures, etc.).\n",
        "- **Sliding**: The filter slides over the input image, performing an element-wise multiplication and summation at each position. This operation results in a new 2D array called a **feature map**.\n",
        "\n",
        "## 2. In_channels and Out_channels\n",
        "These terms are crucial when defining a convolutional layer.\n",
        "\n",
        "### a. In_channels:\n",
        "- **Definition**: This specifies the number of input channels in the image being processed.\n",
        "- **Example**:\n",
        "  - For a **grayscale image**, `in_channels` would be `1` (since there is one channel).\n",
        "  - For an **RGB image**, `in_channels` would be `3` (representing the Red, Green, and Blue channels).\n",
        "\n",
        "### b. Out_channels:\n",
        "- **Definition**: This defines how many separate filters (or feature maps) the convolutional layer will learn. Each filter produces one output channel.\n",
        "- **Example**:\n",
        "  - If `out_channels` is set to `256`, then the convolutional layer will apply `256` different filters to the input. Each filter learns to detect different features, and the output will have `256` separate feature maps.\n",
        "\n",
        "## 3. Conv Filters (Convolutional Filters)\n",
        "- **Definition**: Filters (or kernels) are the small matrices (e.g., `3x3`, `5x5`) of weights that are learned during the training process.\n",
        "- **Purpose**: Each filter is designed to detect specific features from the input image:\n",
        "  - For instance, one filter might learn to detect horizontal edges, another might learn vertical edges, and others might detect more complex patterns.\n",
        "\n",
        "### Example of a Filter:\n",
        "```python\n",
        "filter = [[0, -1, 0],\n",
        "          [-1, 4, -1],\n",
        "          [0, -1, 0]]\n",
        "```\n",
        "## 4. Max Pooling\n",
        "\n",
        "### What is a Pooling Layer?\n",
        "A **pooling layer** is used in convolutional neural networks (CNNs) to reduce the spatial dimensions (height and width) of the input feature maps. This is important for several reasons:\n",
        "- **Dimensionality Reduction**: It decreases the number of parameters and computation in the network, making it more efficient.\n",
        "- **Feature Extraction**: It helps retain the most important features while discarding less important information, contributing to better generalization.\n",
        "- **Translation Invariance**: It makes the network more robust to small translations in the input image.\n",
        "\n",
        "### Max Pooling Explained\n",
        "**Max pooling** is one of the most common types of pooling. Here’s how it works:\n",
        "\n",
        "1. **Window/Filter**:\n",
        "   - A pooling layer uses a **window** (or filter) of a certain size, commonly `2x2` or `3x3`.\n",
        "   - For max pooling, this window slides over the input feature map.\n",
        "\n",
        "2. **Sliding the Window**:\n",
        "   - The window moves across the feature map in strides (if `2`, it moves two pixels at a time).\n",
        "   - At each position, it looks at the values contained within the window.\n",
        "\n",
        "3. **Taking the Maximum Value**:\n",
        "   - From the values covered by the window, max pooling selects the **maximum value**.\n",
        "   - This value becomes part of the output feature map at that position.\n",
        "\n",
        "### Example\n",
        "Consider a simple example with a feature map of size `4x4`:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzjdSml-tr5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=2)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=4096, out_features=1024) # Since flattened size of last i.e conv3 is 1024×2×2=4096 (1024 features, pooling gives 2x2 output shape)\n",
        "        self.drop1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=1024)\n",
        "        self.drop2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.out = nn.Linear(in_features=1024, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.pool1(x)\n",
        "\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.pool2(x)\n",
        "\n",
        "      x = F.relu(self.conv3(x))\n",
        "      x = self.pool3(x)\n",
        "\n",
        "      x = self.flatten(x)\n",
        "\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.drop1(x)\n",
        "\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.drop2(x)\n",
        "\n",
        "      x = self.out(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "m_GjWLS1vEQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports:\n",
        "\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "```\n",
        "**torch.nn**: This module contains classes for building neural networks, including layers and activation functions.\n",
        "\n",
        "**torch.nn.functional**: This module provides functions that can be applied to tensors, such as activation functions, pooling operations, etc., without creating a layer object.\n",
        "\n",
        "# NeuralNet Class Definition:\n",
        "\n",
        "\n",
        "```\n",
        "class NeuralNet(nn.Module):\n",
        "```\n",
        "This class inherits from nn.Module, which is the **base class for all neural network modules** in PyTorch. By inheriting from this class, we can define a custom neural network architecture.\n",
        "\n",
        "## init method\n",
        "```\n",
        "def __init__(self):\n",
        "    super().__init__()\n",
        "```\n",
        "This calls the constructor of the parent class (nn.Module), **ensuring proper initialization of the base class.**\n",
        "\n",
        "### Layers\n",
        "\n",
        "```\n",
        "self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3)\n",
        "self.pool1 = nn.MaxPool2d(2, 2)\n",
        "```\n",
        "self.conv1:\n",
        "\n",
        "* **nn.Conv2d**: This defines a 2D convolutional layer.\n",
        "\n",
        "* **in_channels=1**: Input images are grayscale (1 channel).\n",
        "\n",
        "* **out_channels=256**: This layer will output 256 feature maps.\n",
        "* **kernel_size=3**: Each convolution filter is 3x3 pixels.\n",
        "self.pool1:\n",
        "\n",
        "* **nn.MaxPool2d(2, 2)**: This defines a max pooling layer.\n",
        "The pooling layer reduces the spatial dimensions (height and width) of the feature maps by taking the maximum value over a 2x2 window, effectively downsampling the output.\n",
        "\n",
        "# Flattening\n",
        "\n",
        ". Flattening after the third convolutional layer (and its corresponding pooling layer) is essential for transitioning from the convolutional part of the network to the fully connected part. Here's why this step is necessary:\n",
        "\n",
        "**Purpose of Flattening:**\n",
        "\n",
        "**1. Transition from 2D to 1D**:\n",
        " - **Convolutional Layers**: Convolutional layers output a multi-dimensional tensor with the shape `(batch_size, channels, height, width)`. For instance, after the third convolution and pooling, you might have an output shape like `(batch_size, 1024, H, W)`.\n",
        " - **Fully Connected Layers**: Fully connected (dense) layers expect a 1D input for each sample, typically shaped as `(batch_size, features)`. Flattening converts the multi-dimensional tensor into a 1D tensor for each sample, so the input shape becomes `(batch_size, 1024 * H * W)`.\n",
        "\n",
        "**2. Preparing for Classification**:\n",
        " - After extracting features from the images through convolutions and pooling, the network needs to interpret these features to classify the input. Flattening organizes the feature data in a way that the fully connected layers can process it effectively.\n",
        "\n",
        "**3. Maintaining Batch Size**:\n",
        " - Flattening keeps the batch size intact while changing the shape of the individual examples. This means if you have a batch of 32 images, after flattening, each image would be represented as a single vector of features, while still being part of the batch.\n",
        "\n",
        "**Summary**:\n",
        "In essence, flattening after the last convolutional layer is a crucial step that enables the network to transition from feature extraction to classification. It organizes the data into a format suitable for the fully connected layers, allowing the model to learn how to map the extracted features to the correct output classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "wClDJxB2vGKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet()\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdRbyG35xElW",
        "outputId": "164b2052-6d2e-4ced-e88f-dd5c1224912e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (conv1): Conv2d(1, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(512, 1024, kernel_size=(2, 2), stride=(1, 1))\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "  (drop1): Dropout(p=0.3, inplace=False)\n",
              "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (drop2): Dropout(p=0.3, inplace=False)\n",
              "  (out): Linear(in_features=1024, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(trainloader):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "    print(f'input shape: {inputs.shape}')\n",
        "    print(f'after network shape: {net(inputs).shape}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3kQQdJ1yUAm",
        "outputId": "d8b802d5-7a3f-4178-ec34-0cba6a6dbbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([8, 1, 28, 28])\n",
            "after network shape: torch.Size([8, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Looping Through the DataLoader**\n",
        "  - `for i, data in enumerate(trainloader):`\n",
        "    - This loops through the `trainloader`, which is a `DataLoader` object that batches your training data.\n",
        "    - `enumerate` gives both the index `i` (which we don't use in this case) and the `data` for each iteration.\n",
        "\n",
        "- **Extracting Inputs and Labels**\n",
        "  - `inputs, labels = data[0].to(device), data[1].to(device)`\n",
        "    - `data[0]` contains the batch of input images, while `data[1]` contains the corresponding labels.\n",
        "    - `to(device)` moves the tensors to the specified device (usually GPU if available).\n",
        "  \n",
        "- **Input Shape**\n",
        "  - `print(f'input shape: {inputs.shape}')`\n",
        "    - The output shows `input shape: torch.Size([8, 1, 28, 28])`.\n",
        "    - This means:\n",
        "      - **`8`**: There are `8` images in this batch.\n",
        "      - **`1`**: Each image has `1` channel (grayscale).\n",
        "      - **`28` x `28`**: Each image has a height and width of `28` pixels.\n",
        "\n",
        "- **Passing Through the Network**\n",
        "  - `print(f'after network shape: {net(inputs).shape}')`\n",
        "    - `net(inputs)` passes the batch of input images through the neural network defined earlier.\n",
        "    - The output shape is `torch.Size([8, 10])`.\n",
        "    - This means:\n",
        "      - **`8`**: The batch size remains `8` after passing through the network.\n",
        "      - **`10`**: The network outputs `10` values for each input image, which typically correspond to the probabilities or scores for each of the `10` classes in the FashionMNIST dataset.\n",
        "\n",
        "## Summary\n",
        "- The input to the network is a batch of `8` grayscale images, each of size `28x28`.\n",
        "- After processing through the network, the output is a batch of `8` vectors, each containing `10` values representing the predicted class scores for each image. This transformation is typical in classification tasks, where the model outputs a score for each possible class."
      ],
      "metadata": {
        "id": "DtBCXtS_1Lcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = 0\n",
        "for x in net.parameters():\n",
        "  num_params += len(torch.flatten(x))\n",
        "\n",
        "print(f'Number of parameters in the model: {num_params:,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiWkxal8016J",
        "outputId": "3980a6b2-a151-4336-f1a0-471f18dadec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in the model: 8,536,074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are \"Net Parameters\"?**\n",
        "\n",
        "1. **Neural Network Structure**:\n",
        "- A neural network is made up of layers, which contain **neurons** (or nodes).\n",
        "- Each neuron in one layer can connect to neurons in the next layer, and these connections have **weights**.\n",
        "\n",
        "2. **Parameters**:\n",
        "- **Parameters** in a neural network refer to:\n",
        "  - **Weights**: Values that are multiplied by the inputs to determine how much influence an input has on the neuron's output.\n",
        "  - **Biases**: Additional values added to the output of a neuron to adjust the activation.\n",
        "\n",
        "3. **Counting Parameters**:\n",
        "- The total number of parameters in a network includes all the weights and biases across all layers.\n",
        "\n",
        "**The Code Explanation**:\n",
        "\n",
        "- **Purpose**: The code counts how many parameters (weights and biases) are in the neural network called `net`.\n",
        "\n",
        "**Step-by-Step**:\n",
        "\n",
        "1. **Initialization**:\n",
        "  ```python\n",
        "  num_params = 0\n",
        "  ```\n",
        "  - Start with zero parameters.\n",
        "\n",
        "2. **Loop Through Parameters**:\n",
        "  ```python\n",
        "  for x in net.parameters():\n",
        "  ```\n",
        "  - For each parameter (weight or bias) in the network:\n",
        "    - **`x`** represents the current parameter tensor.\n",
        "\n",
        "3. **Count Parameters**:\n",
        "  ```python\n",
        "  num_params += len(torch.flatten(x))\n",
        "  ```\n",
        "  - Flatten the parameter tensor into a 1D array and count how many elements it has (this gives the number of parameters).\n",
        "  - Add this count to the total `num_params`.\n",
        "\n",
        "4. **Print the Total**:\n",
        "  ```python\n",
        "  print(f'Number of parameters in the model: {num_params:,}')\n",
        "  ```\n",
        "  - Print the total number of parameters in a readable format, with commas for clarity.\n",
        "\n",
        "## Question:\n",
        "How does a neural network have parameters already if the model did not get any input?\n",
        "\n",
        "## Answer:\n",
        "In neural networks, parameters (weights and biases) are initialized when you define the model architecture, not when you provide input data. Here’s how it works:\n",
        "\n",
        "### Initialization of Parameters:\n",
        "\n",
        "1. **Model Definition**:\n",
        "   - When you create a neural network model (e.g., by defining a class that inherits from `nn.Module`), you specify the layers and their configurations (like the number of neurons, types of layers, etc.).\n",
        "   - For example, in your `NeuralNet` class, when you define layers like `self.conv1`, `self.fc1`, etc., you are setting up the structure of the model.\n",
        "\n",
        "2. **Automatic Parameter Initialization**:\n",
        "   - Once you define the layers, PyTorch automatically initializes the weights and biases for these layers using default strategies (like random values or specific distributions).\n",
        "   - This means that as soon as you create an instance of your model, it already has parameters, even if it hasn't seen any input data yet.\n",
        "\n",
        "3. **Training Process**:\n",
        "   - During training, these parameters are adjusted based on the input data and the loss calculated from the model's predictions. The optimizer updates the parameters to minimize the loss, but the initial parameters exist prior to any training.\n",
        "\n"
      ],
      "metadata": {
        "id": "BOObre5Q7KMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer and Loss Function"
      ],
      "metadata": {
        "id": "HoTQIjE9-qfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim #contains various optimization algorithms\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #since categorical\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "Zudpjxyx6_9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n"
      ],
      "metadata": {
        "id": "mJzykk92Ay2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "  net.train(True) #Training mode\n",
        "\n",
        "  #These variables will accumulate the loss and accuracy for each batch processed.\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "\n",
        "  for batch_index, data in enumerate(trainloader):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Forward Pass\n",
        "    outputs = net(inputs) # shape: [batch_size, 10], i.e 8x10, i.e for each image in the batch model outputs 10 values since there are 10 classes\n",
        "    correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
        "    '''\n",
        "    For each item the model outputs an array of 10 numbers because the output layer we set to 10 because we had\n",
        "    10 classes. Now let's say the 8th number in that array is the highest, it simply means the model thinks\n",
        "    the image corresponds to 8th label.\n",
        "\n",
        "    torch.argmax(outputs, dim=1) gives the predicted class for each input.\n",
        "    labels == ... compares these predictions with the true labels.\n",
        "    The accuracy for the batch is calculated and accumulated.\n",
        "\n",
        "    torch.sum() sums the boolean tensor, treating True as 1 and False as 0.\n",
        "    This gives you the total count of correctly predicted labels in the current batch.\n",
        "    '''\n",
        "\n",
        "\n",
        "    running_accuracy += correct / batch_size\n",
        "\n",
        "    loss = criterion(outputs, labels) #The loss is calculated for the batch and added to running_loss\n",
        "    running_loss += loss.item()\n",
        "\n",
        "\n",
        "    #Back Propagation\n",
        "    loss.backward() #computes the gradient of the loss with respect to model parameters.\n",
        "    optimizer.step() #updates the model parameters based on the calculated gradients.\n",
        "\n",
        "    #Logging Progress\n",
        "    if batch_index % 500 == 499:  # print every 500 batches\n",
        "      avg_loss_across_batches = running_loss / 500\n",
        "      avg_acc_across_batches = (running_accuracy / 500) * 100\n",
        "      print('Batch {0}, Loss: {1:.3f}, Accuracy: {2:.1f}%'.format(batch_index+1,\n",
        "                                                          avg_loss_across_batches,\n",
        "                                                          avg_acc_across_batches))\n",
        "      running_loss = 0.0\n",
        "      running_accuracy = 0.0\n",
        "\n",
        "\n",
        "  print()\n",
        "\n",
        "def validate_one_epoch():\n",
        "  net.train(False)\n",
        "  running_loss = 0.0\n",
        "  running_accuracy = 0.0\n",
        "\n",
        "  for i, data in enumerate(valloader):\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "      with torch.no_grad(): #we’re only interested in predictions, not training.\n",
        "          outputs = net(inputs) # shape: [batch_size, 10]\n",
        "          correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
        "          running_accuracy += correct / batch_size\n",
        "          loss = criterion(outputs, labels) # One number, the average batch loss\n",
        "          running_loss += loss.item()\n",
        "\n",
        "  avg_loss_across_batches = running_loss / len(valloader) #len valloader is the number of batches in the validation set\n",
        "  avg_acc_across_batches = (running_accuracy / len(valloader)) * 100\n",
        "\n",
        "  print('Val Loss: {0:.3f}, Val Accuracy: {1:.1f}%'.format(avg_loss_across_batches,\n",
        "                                                          avg_acc_across_batches))\n",
        "  print('***************************************************')\n",
        "  print()"
      ],
      "metadata": {
        "id": "Es8ltsZCAxgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In PyTorch, gradients are accumulated by default. This means that each time you call .backward() on your loss, the gradients of the parameters are added to the existing gradients from previous iterations.**\n",
        "\n",
        "Hence\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "for every batch.\n",
        "\n",
        "A gradient represents the rate of change of a function with respect to its inputs. In the context of machine learning, it indicates how much the loss (or cost) changes when you change the model's parameters (weights and biases).\n",
        "\n",
        "**First order differentials telling us which way to go**\n",
        "\n",
        "### Neural Networks\n",
        "* During training, we want to minimize the loss function (which measures how well the model's predictions match the actual data).\n",
        "* Gradients help us determine how to adjust the parameters to reduce the loss:\n",
        "* If the gradient is positive, increasing the parameter will increase the loss.\n",
        "* If the gradient is negative, increasing the parameter will decrease the loss.\n",
        "* By following the negative gradient (gradient descent), we can find the parameter values that minimize the loss.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rPyk3j-vBmAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch_index in range(num_epochs):\n",
        "    print(f'Epoch: {epoch_index + 1}\\n')\n",
        "\n",
        "    train_one_epoch()\n",
        "    validate_one_epoch()\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f71fvQuTAyJ7",
        "outputId": "625c77de-b559-455c-f20a-9f4246c14555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "Batch 500, Loss: 1.161, Accuracy: 56.9%\n",
            "Batch 1000, Loss: 0.727, Accuracy: 72.0%\n",
            "Batch 1500, Loss: 0.639, Accuracy: 75.8%\n",
            "Batch 2000, Loss: 0.627, Accuracy: 77.0%\n",
            "Batch 2500, Loss: 0.574, Accuracy: 77.8%\n",
            "Batch 3000, Loss: 0.516, Accuracy: 80.6%\n",
            "Batch 3500, Loss: 0.487, Accuracy: 82.1%\n",
            "Batch 4000, Loss: 0.486, Accuracy: 81.9%\n",
            "Batch 4500, Loss: 0.468, Accuracy: 82.9%\n",
            "Batch 5000, Loss: 0.446, Accuracy: 83.5%\n",
            "Batch 5500, Loss: 0.410, Accuracy: 85.5%\n",
            "Batch 6000, Loss: 0.426, Accuracy: 84.2%\n",
            "\n",
            "Val Loss: 0.374, Val Accuracy: 85.4%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "Batch 500, Loss: 0.397, Accuracy: 84.8%\n",
            "Batch 1000, Loss: 0.365, Accuracy: 86.6%\n",
            "Batch 1500, Loss: 0.378, Accuracy: 85.9%\n",
            "Batch 2000, Loss: 0.351, Accuracy: 86.8%\n",
            "Batch 2500, Loss: 0.355, Accuracy: 87.3%\n",
            "Batch 3000, Loss: 0.338, Accuracy: 87.4%\n",
            "Batch 3500, Loss: 0.343, Accuracy: 87.2%\n",
            "Batch 4000, Loss: 0.339, Accuracy: 88.0%\n",
            "Batch 4500, Loss: 0.332, Accuracy: 87.8%\n",
            "Batch 5000, Loss: 0.325, Accuracy: 88.1%\n",
            "Batch 5500, Loss: 0.319, Accuracy: 87.9%\n",
            "Batch 6000, Loss: 0.346, Accuracy: 87.5%\n",
            "\n",
            "Val Loss: 0.305, Val Accuracy: 89.1%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "Batch 500, Loss: 0.289, Accuracy: 89.2%\n",
            "Batch 1000, Loss: 0.314, Accuracy: 88.5%\n",
            "Batch 1500, Loss: 0.308, Accuracy: 88.5%\n",
            "Batch 2000, Loss: 0.294, Accuracy: 89.8%\n",
            "Batch 2500, Loss: 0.281, Accuracy: 89.8%\n",
            "Batch 3000, Loss: 0.276, Accuracy: 89.8%\n",
            "Batch 3500, Loss: 0.287, Accuracy: 90.0%\n",
            "Batch 4000, Loss: 0.303, Accuracy: 89.0%\n",
            "Batch 4500, Loss: 0.287, Accuracy: 89.6%\n",
            "Batch 5000, Loss: 0.270, Accuracy: 90.2%\n",
            "Batch 5500, Loss: 0.277, Accuracy: 89.4%\n",
            "Batch 6000, Loss: 0.260, Accuracy: 90.9%\n",
            "\n",
            "Val Loss: 0.287, Val Accuracy: 89.6%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 4\n",
            "\n",
            "Batch 500, Loss: 0.253, Accuracy: 91.1%\n",
            "Batch 1000, Loss: 0.247, Accuracy: 90.8%\n",
            "Batch 1500, Loss: 0.239, Accuracy: 91.1%\n",
            "Batch 2000, Loss: 0.257, Accuracy: 90.8%\n",
            "Batch 2500, Loss: 0.230, Accuracy: 91.7%\n",
            "Batch 3000, Loss: 0.272, Accuracy: 90.0%\n",
            "Batch 3500, Loss: 0.238, Accuracy: 91.3%\n",
            "Batch 4000, Loss: 0.265, Accuracy: 90.0%\n",
            "Batch 4500, Loss: 0.238, Accuracy: 91.8%\n",
            "Batch 5000, Loss: 0.261, Accuracy: 90.7%\n",
            "Batch 5500, Loss: 0.255, Accuracy: 91.0%\n",
            "Batch 6000, Loss: 0.228, Accuracy: 92.0%\n",
            "\n",
            "Val Loss: 0.251, Val Accuracy: 90.8%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 5\n",
            "\n",
            "Batch 500, Loss: 0.222, Accuracy: 91.7%\n",
            "Batch 1000, Loss: 0.183, Accuracy: 93.0%\n",
            "Batch 1500, Loss: 0.200, Accuracy: 92.8%\n",
            "Batch 2000, Loss: 0.217, Accuracy: 92.2%\n",
            "Batch 2500, Loss: 0.253, Accuracy: 91.0%\n",
            "Batch 3000, Loss: 0.209, Accuracy: 92.2%\n",
            "Batch 3500, Loss: 0.219, Accuracy: 91.9%\n",
            "Batch 4000, Loss: 0.217, Accuracy: 91.7%\n",
            "Batch 4500, Loss: 0.233, Accuracy: 91.2%\n",
            "Batch 5000, Loss: 0.225, Accuracy: 92.0%\n",
            "Batch 5500, Loss: 0.213, Accuracy: 91.8%\n",
            "Batch 6000, Loss: 0.223, Accuracy: 92.2%\n",
            "\n",
            "Val Loss: 0.261, Val Accuracy: 90.1%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 6\n",
            "\n",
            "Batch 500, Loss: 0.195, Accuracy: 92.8%\n",
            "Batch 1000, Loss: 0.188, Accuracy: 92.9%\n",
            "Batch 1500, Loss: 0.200, Accuracy: 92.8%\n",
            "Batch 2000, Loss: 0.195, Accuracy: 93.2%\n",
            "Batch 2500, Loss: 0.183, Accuracy: 93.0%\n",
            "Batch 3000, Loss: 0.189, Accuracy: 92.9%\n",
            "Batch 3500, Loss: 0.194, Accuracy: 93.1%\n",
            "Batch 4000, Loss: 0.190, Accuracy: 92.8%\n",
            "Batch 4500, Loss: 0.183, Accuracy: 93.0%\n",
            "Batch 5000, Loss: 0.190, Accuracy: 92.8%\n",
            "Batch 5500, Loss: 0.210, Accuracy: 91.9%\n",
            "Batch 6000, Loss: 0.201, Accuracy: 92.1%\n",
            "\n",
            "Val Loss: 0.242, Val Accuracy: 91.4%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 7\n",
            "\n",
            "Batch 500, Loss: 0.160, Accuracy: 93.7%\n",
            "Batch 1000, Loss: 0.150, Accuracy: 93.9%\n",
            "Batch 1500, Loss: 0.166, Accuracy: 93.6%\n",
            "Batch 2000, Loss: 0.170, Accuracy: 94.0%\n",
            "Batch 2500, Loss: 0.166, Accuracy: 93.8%\n",
            "Batch 3000, Loss: 0.165, Accuracy: 93.8%\n",
            "Batch 3500, Loss: 0.166, Accuracy: 93.9%\n",
            "Batch 4000, Loss: 0.178, Accuracy: 93.5%\n",
            "Batch 4500, Loss: 0.186, Accuracy: 93.2%\n",
            "Batch 5000, Loss: 0.165, Accuracy: 93.7%\n",
            "Batch 5500, Loss: 0.177, Accuracy: 93.5%\n",
            "Batch 6000, Loss: 0.177, Accuracy: 93.6%\n",
            "\n",
            "Val Loss: 0.234, Val Accuracy: 91.6%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 8\n",
            "\n",
            "Batch 500, Loss: 0.143, Accuracy: 94.5%\n",
            "Batch 1000, Loss: 0.152, Accuracy: 94.3%\n",
            "Batch 1500, Loss: 0.146, Accuracy: 94.4%\n",
            "Batch 2000, Loss: 0.156, Accuracy: 94.2%\n",
            "Batch 2500, Loss: 0.138, Accuracy: 94.7%\n",
            "Batch 3000, Loss: 0.150, Accuracy: 94.5%\n",
            "Batch 3500, Loss: 0.146, Accuracy: 94.0%\n",
            "Batch 4000, Loss: 0.144, Accuracy: 94.6%\n",
            "Batch 4500, Loss: 0.142, Accuracy: 94.8%\n",
            "Batch 5000, Loss: 0.155, Accuracy: 94.5%\n",
            "Batch 5500, Loss: 0.147, Accuracy: 94.5%\n",
            "Batch 6000, Loss: 0.143, Accuracy: 94.8%\n",
            "\n",
            "Val Loss: 0.239, Val Accuracy: 91.7%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 9\n",
            "\n",
            "Batch 500, Loss: 0.125, Accuracy: 95.5%\n",
            "Batch 1000, Loss: 0.132, Accuracy: 95.2%\n",
            "Batch 1500, Loss: 0.120, Accuracy: 95.4%\n",
            "Batch 2000, Loss: 0.130, Accuracy: 94.5%\n",
            "Batch 2500, Loss: 0.127, Accuracy: 95.2%\n",
            "Batch 3000, Loss: 0.130, Accuracy: 95.3%\n",
            "Batch 3500, Loss: 0.118, Accuracy: 95.5%\n",
            "Batch 4000, Loss: 0.119, Accuracy: 95.5%\n",
            "Batch 4500, Loss: 0.127, Accuracy: 95.1%\n",
            "Batch 5000, Loss: 0.132, Accuracy: 94.9%\n",
            "Batch 5500, Loss: 0.133, Accuracy: 94.9%\n",
            "Batch 6000, Loss: 0.131, Accuracy: 95.0%\n",
            "\n",
            "Val Loss: 0.289, Val Accuracy: 91.8%\n",
            "***************************************************\n",
            "\n",
            "Epoch: 10\n",
            "\n",
            "Batch 500, Loss: 0.104, Accuracy: 96.0%\n",
            "Batch 1000, Loss: 0.115, Accuracy: 95.8%\n",
            "Batch 1500, Loss: 0.120, Accuracy: 95.4%\n",
            "Batch 2000, Loss: 0.101, Accuracy: 96.2%\n",
            "Batch 2500, Loss: 0.121, Accuracy: 95.4%\n",
            "Batch 3000, Loss: 0.110, Accuracy: 95.9%\n",
            "Batch 3500, Loss: 0.111, Accuracy: 96.0%\n",
            "Batch 4000, Loss: 0.109, Accuracy: 95.6%\n",
            "Batch 4500, Loss: 0.104, Accuracy: 96.0%\n",
            "Batch 5000, Loss: 0.114, Accuracy: 95.9%\n",
            "Batch 5500, Loss: 0.100, Accuracy: 96.3%\n",
            "Batch 6000, Loss: 0.117, Accuracy: 95.8%\n",
            "\n",
            "Val Loss: 0.266, Val Accuracy: 91.6%\n",
            "***************************************************\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQjHIwEBIarS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}